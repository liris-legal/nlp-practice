{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 言語処理課題\n",
    "---\n",
    "課題についての諸注意\n",
    "- 特に指示のないものについてはセルを実行してください。(セルの実行はセルを選択して Shift+Enter)\n",
    "- 特に指示のない箇所はコードを変更しないでください。\n",
    "- docstringsやコメントを参照し、要件を満たしたコード、テキストを書いてください。\n",
    "- importされているライブラリは利用してください。\n",
    "- 別途ライブラリをimportしても構いませんが、Dockerコンテナに事前にインストールされているもの以外は使わないでください。\n",
    "- 十分に調べ、考えてもわからない場合は回答例をsrc/script/answer_example/answer_example.ipynbを見てください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### ファイルの読み込み\n",
    "目標：いろいろな拡張子のファイルを読み込めるようになる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# このセルをShift+Enterで実行してください\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from typing import Dict, List, Union\n",
    "from pprint import pprint\n",
    "\n",
    "DATAPATH = os.getenv('DATAPATH')\n",
    "TEST_TEXT_FILE_PATH = os.path.join(DATAPATH, 'aozora_samples/aibiki.txt')\n",
    "TEST_CSV_FILE_PATH = os.path.join(DATAPATH, 'test_read_file/test_read.csv')\n",
    "TEST_TSV_FILE_PATH = os.path.join(DATAPATH, 'test_read_file/test_read.tsv')\n",
    "TEST_JSON_FILE_PATH = os.path.join(DATAPATH, 'chABSA_dataset/e00008_ann.json')\n",
    "TEST_DOCX_FILE_PATH = os.path.join(DATAPATH, 'contract_samples/01-売買基本契約書.docx')\n",
    "TEST_PDF_FILE_PATH = os.path.join(DATAPATH, 'contract_samples/02-OEM契約書.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### No.1: テキストファイルを読み込む関数を作成してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "from chardet.universaldetector import UniversalDetector\n",
    "\n",
    "def detect_encoding(file_path: str, chunk_size: int = 100) -> Dict[str, Union[str, float]]:\n",
    "    \"\"\"エンコード方式を自動で検出\n",
    "    - 問: の部分にしたがって回答してください\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        対象のファイルパス\n",
    "    chunk_size : int, optional\n",
    "        1回の分析の最大容量（Byte単位）, by default 100\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Union[str, float]]\n",
    "        {'encoding': 'エンコード内容', 'confidence': '検出の確度'}\n",
    "    \"\"\"\n",
    "    # --- 以下コメント通りに回答 ---\n",
    "    mode = ''  # 問: ファイルをバイナリ形式で読み込めるようにしてください\n",
    "    with open(file_path, mode) as f, contextlib.closing(UniversalDetector()) as detector:\n",
    "        while True:\n",
    "            chunk = None  # 問: Noneの部分を書き換えて100バイトずつファイルを読み込んでください\n",
    "            # 問: chnkに値が無い場合はbreakするように以下に条件文を書いてください\n",
    "            \n",
    "            detector.feed(chunk)  # チャンクからエンコードを解析\n",
    "            if detector.done:  # 十分な確度で解析できたらbreak\n",
    "                break\n",
    "    return detector.result\n",
    "\n",
    "def read_txt(file_path: str, encoding: str = 'utf-8') -> str:\n",
    "    \"\"\"txtファイルの読み込み\n",
    "    - encodingを用いてテキストファイルを読み込んでください\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        読み込む対象のファイルパス\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        テキストファイルを読み込んだ結果\n",
    "    \"\"\"\n",
    "    encoding = detect_encoding(file_path=file_path)['encoding']  # ここは変えない\n",
    "    # --- 以下に回答 ---\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下を実行してErrorにならないか確認\n",
    "result_utf8 = read_txt(file_path=TEST_TEXT_FILE_PATH)\n",
    "assert result_utf8[:4] == 'あいびき'\n",
    "print(f'result_utf8[:4]: {result_utf8[:4]}')\n",
    "print('合格!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### No.2: CSV, TSVファイルの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def read_separated_value_file(file_path: str, delimiter: str = ',', encoding: str = 'utf-8') -> List[List[str]]:\n",
    "    \"\"\"CSVやTSVファイルなどの区切り文字が指定されているファイルの読み込み\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        読み込む対象のファイルパス\n",
    "    delimiter : str, optional\n",
    "        区切り文字, by default \",\"\n",
    "    encoding : str, optional\n",
    "        テキストファイルのエンコード方式を指定, by default \"utf-8\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[List[str]]\n",
    "        ファイルを読み込んだ結果を2次元の配列で取得\n",
    "    \"\"\"\n",
    "    # --- 以下に回答 ---\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# このセルを実行してCSV、TSVファイルの中身がきちんと読み込まれているか確認してください。\n",
    "result_csv = read_separated_value_file(file_path=TEST_CSV_FILE_PATH, delimiter=',')\n",
    "result_tsv = read_separated_value_file(file_path=TEST_TSV_FILE_PATH, delimiter='\\t')\n",
    "\n",
    "print(f'result_csv: {result_csv}')\n",
    "if result_csv == [['0', 'c'], ['1', 's'], ['2', 'v']]:\n",
    "    print('CSV読み込み: 合格')\n",
    "else:\n",
    "    print('CSV読み込み: まだまだです')\n",
    "print()\n",
    "print(f'result_tsv: {result_tsv}')\n",
    "if result_tsv == [['0', 'T'], ['1', 'S'], ['2', 'V']]:\n",
    "    print('TSV読み込み: 合格')\n",
    "else:\n",
    "    print('TSV読み込み: まだまだです')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### No.3: JSONファイルの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json(file_path: str) -> dict:\n",
    "    \"\"\"JSONファイルの読み込み\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        ファイルパス\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Jsonファイルを読み込んだ結果\n",
    "    \"\"\"\n",
    "    # --- 以下に回答 ---\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下を実行してErrorにならないか確認\n",
    "result_json = read_json(file_path=TEST_JSON_FILE_PATH)\n",
    "assert result_json['header']['category17'] == '食品'\n",
    "pprint(result_json['header'])\n",
    "print('合格!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### No.4（Challenge！）: docxファイルの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "def read_docx_text(file_path: str) -> List[str]:\n",
    "    \"\"\"docxのWordファイルの読み込み\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        ファイルパス\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        docxを段落ごとに分割して読み込んだ結果\n",
    "    \"\"\"\n",
    "    # --- 以下に回答 ---\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下を実行してdocxファイルの中身を確認してください。\n",
    "result_docx = read_docx_text(file_path=TEST_DOCX_FILE_PATH)\n",
    "\n",
    "answer_list = [\n",
    "    '売買基本契約書',\n",
    "    '鈴木株式会社（以下「甲」という。）とダイヤモンド株式会社（以下「乙」という。）は，乙が取り扱う商品（以下「本製品」という）の売買に関し，基本的事項を定めるため，以下のとおり契約（以下「本契約」という）を締結する。'\n",
    "]\n",
    "print(f'result_docx[:2]:\\n{result_docx[:2]}')\n",
    "if result_docx[:2] == answer_list:\n",
    "    print('合格!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### No.5（Challenge！!）: pdfファイルの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter, PDFResourceManager\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "\n",
    "def read_pdf_text(file_path: str, password: str = None) -> List[str]:\n",
    "    \"\"\"PDFファイルからテキストを読み込む\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        ファイルパス\n",
    "    password : str, optional\n",
    "        PDFファイルにパスワードがかかっている場合はそのパスワード, by default None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        抽出したPDFのテキスト情報を改行コードごとに分割したリスト\n",
    "    \"\"\"\n",
    "    # --- 以下に回答 ---\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下を実行してエラーにならないか確認してください。\n",
    "result_pdf = read_pdf_text(file_path=TEST_PDF_FILE_PATH)\n",
    "assert result_pdf[:6] == 'ＯＥＭ契約書'\n",
    "print(result_pdf[:6])\n",
    "print('合格!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### 基本的なテキストの前処理\n",
    "目標：基本的な前処理を実施できるようにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# このセルをShift+Enterで実行してください\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import re\n",
    "from typing import List, Union\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### No.6: 全角半角の統一\n",
    "- 同じ意味のカタカナや数字なのに、半角と全角で異なる単語として認識されることを防ぎます\n",
    "- ここではmojimojiというライブラリを用いて半角と全角を統一します\n",
    "- このセクションは問いはありませんが、使えるようにしてください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mojimoji\n",
    "zenkaku_text = \"あいうＡＩＵアイウ０１２＋ー＝（）\"\n",
    "print(f'zenkaku_text: {zenkaku_text}')\n",
    "\n",
    "# 半角にできる文字が半角にできることを確認\n",
    "hankaku_text = mojimoji.zen_to_han(zenkaku_text)\n",
    "print(f'hankaku_text: {hankaku_text}')\n",
    "\n",
    "# 半角にした文字を全角に戻せることを確認\n",
    "hankaku_to_zenkaku_text = mojimoji.han_to_zen(hankaku_text)\n",
    "print(f'bool(hankaku_to_zenkaku_text==zenkaku_text): {bool(hankaku_to_zenkaku_text == zenkaku_text)}')\n",
    "\n",
    "# 数字だけ半角にする\n",
    "print(f'数字だけ半角: {mojimoji.zen_to_han(zenkaku_text, kana=False, digit=True, ascii=False)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### No.7: 正規表現を用いた文字列の置換・削除\n",
    "- よくある文字の置換処理: 連続する空白文字の削除、文章前後の空白文字の削除、連続する数字を0などに置換、表記ゆれの統一を行います\n",
    "- 正規表現を頻繁に使いますので読み慣れておくと良いです。\n",
    "- Pythonで正規表現を用いて文字列を処理する場合はreというライブラリを用います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 問: question_price_textがanswer_price_textになるように正規表現をunify_number_patternに記入してください\n",
    "unify_number_pattern = ''  # ←ここに正規表現を書く\n",
    "\n",
    "# 以下触らない\n",
    "question_price_text = '500円、1,000円、3,000,000円、50,000,000円'\n",
    "answer_price_text = '0円、0円、0円、0円'\n",
    "\n",
    "my_answer = re.sub(unify_number_pattern, '0', question_price_text)\n",
    "\n",
    "print(f'question_price_text: {question_price_text}')\n",
    "print(f'my_answer: {my_answer}')\n",
    "\n",
    "if bool(my_answer == answer_price_text):\n",
    "    print('合格！')\n",
    "else:\n",
    "    print('まだまだです')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 問: question_price_textがanswer_price_textになるように正規表現をunify_number_patternに記入してください\n",
    "spaces_pattern = ''  # ←ここに正規表現を書く\n",
    "\n",
    "# 以下触らない\n",
    "question_space_text = '  スペ  ー スは見　　\\n　え　\\t　ない敵 '\n",
    "answer_space_text = 'スペースは見えない敵'\n",
    "print(f'question_space_text:\\n{question_space_text}')\n",
    "\n",
    "my_answer = re.sub(spaces_pattern, '', question_space_text)\n",
    "\n",
    "print(f'my_answer: {my_answer}')\n",
    "if bool(my_answer == answer_space_text):\n",
    "    print('合格！')\n",
    "else:\n",
    "    print('まだまだです')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### 文章の分かち書きと形態素解析\n",
    "目標：日本語は英語と違い、単語がスペース等で区切られていないので、文章を単語（形態素）に分割する処理（分かち書き）が必要です。その分かち書きができるようになることが目標です。ついでに、単語に分割する際に形態素解析器を使えるようにします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### No.8: 形態素解析器を用いた分かち書き\n",
    "- 日本語の形態素解析器はいくつか種類がありますが、今回は導入が簡単なJanomeとspacyでラップされたGinzaという形態素解析器を用いて分かち書きを行います。\n",
    "- JanomeはMeCabベースの形態素解析器です。pipでインストールできるので導入が楽です。MeCab単体よりも動作は少し遅いですが、数千行程度の形態素解析であれば導入が楽なJanomeが良いでしょう。\n",
    "- Ginzaは形態素解析のベースとしてSudachiを用いています。Spacyというフレームワークでラップされています。形態素解析以外の機能が豊富で、導入も楽です。精度はMeCabと同程度で、速度はMeCabよりも少し遅い程度です。固有表現抽出や係り受け解析など他の解析もできるのでいろいろ分析する場合はGinzaが良いでしょう。\n",
    "- このセクションに問いはありませんが、使えるようにしてください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Janomeを用いた形態素解析と分かち書き\n",
    "from janome.tokenizer import Tokenizer as JanomeTokenizer\n",
    "janome_tokenizer = JanomeTokenizer()\n",
    "\n",
    "sentence = '他者と比べるのではなく、過去の自分と比べなさい'\n",
    "\n",
    "# シンプルに分かち書きをする場合（見出し語の分かち書き）\n",
    "janome_wakachi_result = list(janome_tokenizer.tokenize(sentence, wakati=True))\n",
    "print(f'見出し語の分かち書き:\\n{janome_wakachi_result}')\n",
    "\n",
    "# 単語の原型を用いた分かち書き\n",
    "tokens = janome_tokenizer.tokenize(sentence)\n",
    "janome_base_form_wakachi_result = [t.base_form for t in tokens]\n",
    "print(f'原型での分かち書き:\\n{janome_base_form_wakachi_result}')\n",
    "\n",
    "# 単語の品詞を見てみる\n",
    "tokens = janome_tokenizer.tokenize(sentence)\n",
    "pos_result = [t.part_of_speech for t in tokens]\n",
    "print(f'品詞:\\n{pos_result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ginzaを用いた形態素解析と分かち書き\n",
    "import spacy\n",
    "ginza_tokenizer = spacy.load(\"ja_ginza\")\n",
    "\n",
    "sentence = '他者と比べるのではなく、過去の自分と比べなさい'\n",
    "\n",
    "tokens = ginza_tokenizer(sentence)\n",
    "\n",
    "# シンプルに分かち書きをする場合（見出し語の分かち書き）\n",
    "ginza_wakachi_result = [token.orth_ for token in tokens]\n",
    "print(f'見出し語の分かち書き:\\n{ginza_wakachi_result}')\n",
    "\n",
    "# 単語の原型を用いた分かち書き\n",
    "ginza_base_form_wakachi_result = [token.lemma_ for token in tokens]\n",
    "print(f'原型での分かち書き:\\n{ginza_base_form_wakachi_result}')\n",
    "\n",
    "# 単語の品詞を見てみる\n",
    "pos_result = [token.pos_ for token in tokens]\n",
    "print(f'品詞:\\n{pos_result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### No.9: N-gram\n",
    "- 形態素解析器を用いず、文字をN文字の小さな塊として分割する手法があります。\n",
    "- N-gramという考え方があり、Nは文字列の塊に何文字含めるかを示します。\n",
    "- N-gramには文字レベルのものと単語レベルのものがあります。文字で塊を作るか、単語で塊を作るかの違いがあります。単語レベルの場合は形態素解析が必要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ngram(item: Union[str, List[str]], n: int) -> List[str]:\n",
    "    \"\"\"N-gram変換器\n",
    "    - 文字列'これはテスト'が入力された場合の出力例\n",
    "        - N=1の場合の期待される出力: ['こ', 'れ', 'は', 'テ', 'ス', 'ト']\n",
    "        - N=2の場合の期待される出力: ['これ', 'れは', 'はテ', 'テス', 'スト']\n",
    "        - N=3の場合の期待される出力: ['これは', 'れはテ', 'はテス', 'テスト']\n",
    "    - 単語の配列['これ', 'は', 'テスト']が入力された場合の出力例\n",
    "        - N=1の場合の期待される出力: [['これ'], ['は'], ['テスト']]\n",
    "        - N=2の場合の期待される出力: [['これ', 'は'], ['は', 'テスト']]\n",
    "        - N=3の場合の期待される出力: [['これ', 'は', 'テスト']]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    item : Union[str, List[str]]\n",
    "        N-gramに変換したい文字列、または分かち書きのリスト\n",
    "    n : int\n",
    "        N-gramのNに相当する数\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        itemをN-gramに変換した結果\n",
    "    \"\"\"\n",
    "    if n < 1:\n",
    "        raise Exception(f\"n > 0 (but n={n})\")\n",
    "    # --- 以下に回答 ---\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作成したto_ngram関数の確認。すべてエラーにならなければ合格。\n",
    "from src.nlp.tokenizer import to_ngram\n",
    "\n",
    "test_sentence = 'これはテスト'\n",
    "test_list = ['これ', 'は', 'テスト']\n",
    "\n",
    "assert to_ngram(test_sentence, 1) == ['こ', 'れ', 'は', 'テ', 'ス', 'ト']\n",
    "assert to_ngram(test_sentence, 2) == ['これ', 'れは', 'はテ', 'テス', 'スト']\n",
    "assert to_ngram(test_sentence, 3) == ['これは', 'れはテ', 'はテス', 'テスト']\n",
    "\n",
    "assert to_ngram(test_list, 1) == [['これ'], ['は'], ['テスト']]\n",
    "assert to_ngram(test_list, 2) == [['これ', 'は'], ['は', 'テスト']]\n",
    "assert to_ngram(test_list, 3) == [['これ', 'は', 'テスト']]\n",
    "\n",
    "print('合格!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### No.10: Sentencepiece\n",
    "- Sentencepieceは教師なし学習によって文章の分割方法を学習してます\n",
    "- 言葉の意味は考慮していないので形態素解析のように意味のある単語に分割するわけではないので注意\n",
    "- 高頻度語は一つの単語として認識し、低頻度語は文字や部分文字列にする処理を行うので未知語が出にくいというのがメリット\n",
    "- 今回は日本語Wikipediaを学習させたモデルをロードして使用します\n",
    "- このセクションは問いはありませんが、使えるようにしてください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.functional import load_sp_model, sentencepiece_tokenizer\n",
    "from src.constants import SENTENCE_PIECE_MODEL_PATH\n",
    "\n",
    "sp_model = load_sp_model(SENTENCE_PIECE_MODEL_PATH)\n",
    "sp_tokens_generator = sentencepiece_tokenizer(sp_model)\n",
    "\n",
    "sentence = '他者と比べるのではなく、過去の自分と比べなさい'\n",
    "\n",
    "sp_result = list(sp_tokens_generator([sentence]))[0]\n",
    "# 文頭、空白は'_'になります\n",
    "print(sp_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### torchtextを用いたデータセット作成と機械学習を用いたテキスト分類\n",
    "- torchtextはpytorchやtensorflowなどのDeep Learningフレームワークで利用される言語処理でやっかいな前処理、データセット作成を簡単にしてくれるお役立ちツール\n",
    "- 今回はpytorchを用いてchABSA_datasetのポジティブ、ネガティブ分類をします。<br>(chABSA_datasetについて: https://github.com/chakki-works/chABSA-dataset)\n",
    "\n",
    "目標：torchtextとPytorchを用いてデータの前処理から機械学習器の学習、評価までの流れを身につける"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchtextの役割について\n",
    "![ImageError](https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F183955%2Fff027e0f-7cd1-6a76-0eaa-05b8ae61074e.png?ixlib=rb-1.2.2&auto=format&gif-q=60&q=75&w=1400&fit=max&s=5f14490ebe9a8903ba827c935ebbd5de)\n",
    "（出典: https://qiita.com/itok_msi/items/1f3746f7e89a19dafac5）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# このセルをShift+Enterで実行してください\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from typing import List, Union\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATAPATH = os.getenv('DATAPATH')\n",
    "chABSA_PATH = os.path.join(DATAPATH, 'chABSA_dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# このセルをShift+Enterで実行してください\n",
    "# chABSA_datasetから今回使うデータだけ抽出しています。\n",
    "\n",
    "def get_chABSA_dataset(directory_path: str) -> List[List[Union[str, int]]]:\n",
    "    \"\"\"chABSA_datasetの読み込み\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory_path : str\n",
    "        対象のファイルパス\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[List[Union[str, int]]]\n",
    "        [[sentence, score],[sentence, score],...,[sentence, score]]となるようにデータを返す\n",
    "        - sentence は result_dict['sentences'][i]['sentence']\n",
    "    \"\"\"\n",
    "    dataset_list = []\n",
    "    file_path_list = glob.glob(directory_path + '*.json')\n",
    "    for file_path in tqdm(file_path_list):\n",
    "        result_dict = read_json(file_path)\n",
    "        for sentence in result_dict['sentences']:\n",
    "            score = 0\n",
    "            for opinion in sentence['opinions']:\n",
    "                polarity = opinion['polarity']\n",
    "                if polarity == 'positive':\n",
    "                    score += 1\n",
    "                elif polarity == 'negative':\n",
    "                    score -= 1\n",
    "            dataset_list.append([sentence['sentence'], score])\n",
    "    return dataset_list\n",
    "\n",
    "chABSA_dataset = get_chABSA_dataset(directory_path=chABSA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# このセルをShift+Enterで実行してください\n",
    "# 最小スコアが-10であるためscoreを+10する\n",
    "for i in range(len(chABSA_dataset)):\n",
    "    chABSA_dataset[i][1] += 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# このセルをShift+Enterで実行してください\n",
    "# chABSA_datasetから今回使うデータを抽出し、データを分割してtsvファイルに書き込み。\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.file_writer import write_in_separated_value_file\n",
    "\n",
    "# 訓練、評価、テスト用にデータを分割\n",
    "train_data, test_valid_data = train_test_split(chABSA_dataset, test_size=0.2, shuffle=True, random_state=12345)\n",
    "valid_data, test_data = train_test_split(test_valid_data, test_size=0.5, shuffle=True, random_state=12345)\n",
    "\n",
    "# ファイルに書き込み\n",
    "write_in_separated_value_file('./train.tsv', train_data, '\\t')\n",
    "write_in_separated_value_file('./test.tsv', test_data, '\\t')\n",
    "write_in_separated_value_file('./valid.tsv', valid_data, '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### No.11: torchtextを用いた機械学習\n",
    "- 機械学習モデルのトレーニング用、学習評価用、テスト用のデータをそれぞれ作成する\n",
    "- ミニバッチを作成することで学習を効率化する\n",
    "- 機械学習器にトレーニングデータを入力して学習させる\n",
    "- 学習後のモデルにテストデータを入力して精度を評価する\n",
    "- このセクションに問いはありませんが、使えるようにしてください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchtext.datasetsのデータ構造イメージ\n",
    "（Exampleクラスは最新バージョンでは廃止され、torchtext.legacyに取り込まれる模様）\n",
    "![ImageError](https://i0.wp.com/hacks.deeplearning.jp/wp-content/uploads/2017/10/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2017-10-17-16.23.46.png?w=1379&ssl=1)\n",
    "(出典: https://hacks.deeplearning.jp/torchtext/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行に少し時間かかります\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from janome.tokenizer import Tokenizer as JanomeTokenizer\n",
    "\n",
    "janome_tokenizer = JanomeTokenizer()\n",
    "nltk.download('stopwords')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 分かち書き関数を定義\n",
    "def tokenize_by_janome(sentence: str) -> List[str]:\n",
    "    return list(janome_tokenizer.tokenize(sentence, wakati=True))\n",
    "\n",
    "# ストップワードを定義\n",
    "def get_stopwords() -> set:\n",
    "    stopword_list = []\n",
    "    eng_stopword_list = stopwords.words('english')\n",
    "    stopword_list += eng_stopword_list\n",
    "    return set(stopword_list)\n",
    "\n",
    "# 分かち書き後の各単語に対する前処理\n",
    "stemmer = PorterStemmer()\n",
    "def stemming(s: str) -> str:\n",
    "    # ステミング (語幹抽出)\n",
    "    return stemmer.stem(s)\n",
    "\n",
    "# Fieldクラスを定義\n",
    "text_field = data.Field(\n",
    "    sequential=True,  # Falseなら、tokenizeしない (デフォルト: True)\n",
    "    use_vocab=True,  # Trueなら、ボキャブラリを使って数値化する\n",
    "    tokenize=tokenize_by_janome,  # 文章を単語などのトークンに分割する処理 (デフォルト: str.split)\n",
    "    lower=True,  # 英文字を小文字に統一 (デフォルト: False)\n",
    "    init_token=None,   # 文頭に付与するトークン (デフォルト: None)\n",
    "    eos_token=None,  # 文末に付与するトークン (デフォルト: None)\n",
    "    fix_length=None,  # 1文あたりのワード数を固定、不足する場合はパディング (デフォルト: None)\n",
    "    pad_first=False,  # パディングを最初にいれるかどうか (デフォルト: False)\n",
    "    truncate_first=False,  # fix_lengthを超えた場合に先頭から単語を削るかどうか (デフォルト: False -> 後ろから削られる)\n",
    "    stop_words=get_stopwords(),  # 除去するストップワード\n",
    "    preprocessing=data.Pipeline(stemming),  # tokenize後の各単語に行う処理 (デフォルト: None)\n",
    "    postprocessing=None,  # ミニバッチ単位で行う処理 (デフォルト: None)\n",
    "    dtype=torch.long,  # データの型 (デフォルト: torch.long)\n",
    "    include_lengths=False,  # パディングした文とあわせて長さを返すかどうか (デフォルト: False)\n",
    "    is_target=False,  # ラベルフィールドかどうか (デフォルト: False) \n",
    "    batch_first=False,  # ミニバッチの次元を最初に追加するかどうか (デフォルト: False)\n",
    ")\n",
    "label_field = data.Field(\n",
    "    sequential=False,\n",
    "    use_vocab=False,\n",
    "    is_target=True\n",
    ")\n",
    "\n",
    "# Fieldクラスの処理順序\n",
    "# 1. UTF-8へのエンコード (Python 2のみ)\n",
    "# 2. tokenize\n",
    "# 3. 小文字化 (lower=Trueのみ)\n",
    "# 4. ストップワードの除去 (stop_wordsに値が設定されている場合のみ)\n",
    "# 5. preprocessingの実行 (preprocessingが設定されている場合のみ)\n",
    "# 上記の順序で処理されるためどういう処理をどこに組み込むべきかを考える必要がある\n",
    "\n",
    "# TSVファイルからDatasetにロード\n",
    "train_dataset, valid_dataset, test_dataset = data.TabularDataset.splits(\n",
    "    path=\"./\",  # ファイルまでのパス(相対パス、絶対パスどちらでもOK)\n",
    "    format=\"tsv\",  # tsv, csv, json\n",
    "    fields=[(\"Text\", text_field), (\"Label\", label_field)],  # 読み込むファイルの各列のFieldを指定(今回は1列目に文章, 2列目にラベル)\n",
    "    skip_header=False,  # ヘッダー行がある場合はTrue\n",
    "    train='train.tsv',  # トレーニング用\n",
    "    test='test.tsv',  # テスト用\n",
    "    validation='valid.tsv'  # バリデーション用\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの中身を確認\n",
    "print(f'list(train_dataset.Text)[0]:\\n{list(train_dataset.Text)[0]}\\n')\n",
    "print(f'list(train_dataset.Label)[0]:\\n{list(train_dataset.Label)[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 既存で用意されたVectorizerで単語をベクトル化する場合（今回はTransformerというモデルを使うため不使用）\n",
    "# 初回fasttextの学習済みモデルのダウンロード、セッティングに時間がかかります（10~15分程度）\n",
    "fasttext = torchtext.vocab.FastText(language=\"ja\")\n",
    "\n",
    "# stoi: string to index\n",
    "# itos: index to string\n",
    "print(fasttext.vectors[fasttext.stoi['平成']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fieldクラス内にVocabオブジェクトを作成（単語、出現頻度、単語のIndex情報が入ります）\n",
    "\n",
    "text_field.build_vocab(\n",
    "    train_dataset,  # トレーニング用のデータを入れること。テストデータまで含めるとテストの意味がなくなる。\n",
    "    vectors=fasttext,  # 既存のモデルからベクトル化する場合は指定(デフォルト: None)\n",
    "    min_freq=3,  # min_freq以下でしか出現しない単語を未知語として扱う(デフォルト: 1)\n",
    "    unk_init=None,  # 未知語の場合のベクトル値を指定できる。特に指定なければゼロベクトルとする(デフォルト: None)\n",
    "    specials=['<unk>', '<pad>']  # paddingやeos(end of sentence)などを示すトークンを指定(デフォルト: ['<unk>', '<pad>'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabオブジェクトの確認\n",
    "\n",
    "# 語彙(知っている単語)の確認\n",
    "print(f'語彙の確認:\\n{text_field.vocab.itos[:20]}\\n')\n",
    "\n",
    "# 語彙内のトークンの出現頻度確認\n",
    "print(f'出現頻度の確認:\\n{list(text_field.vocab.freqs.items())[:20]}\\n')\n",
    "\n",
    "# トークンのベクトル化\n",
    "print(f'「平成」の存在確認: {text_field.vocab[\"平成\"]}')  # 0でなければ存在\n",
    "print(f'「平成」ベクトル:\\n{text_field.vocab.vectors[text_field.vocab.stoi[\"平成\"]]}\\n')\n",
    "\n",
    "print(f'「江戸」の存在確認: {text_field.vocab[\"江戸\"]}')  # 0でなければ存在\n",
    "print(f'「江戸」ベクトル:\\n{text_field.vocab.vectors[text_field.vocab.stoi[\"江戸\"]]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# イテレータの作成\n",
    "batch_sizes = (10,5,5)\n",
    "\n",
    "train_iter, valid_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train_dataset, valid_dataset, test_dataset),  # 最初の要素はトレーニング用のデータセットでないといけない\n",
    "    batch_sizes=batch_sizes,  # 1バッチ内に含むデータ数\n",
    "    sort_key=lambda x: len(x.Text),  # バッチ内の単語数のばらつきをなくすために単語数\n",
    ")\n",
    "# トレーニングデータのイテレータには自動的にデータのシャッフルと繰り返し設定が有効となっている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# イテレータの確認\n",
    "train_batch = next(iter(train_iter))\n",
    "\n",
    "# 1バッチ内にbatch_sizes[0]個のデータが入っていることを確認\n",
    "# Iterator内のTextフィールドを見ると中には単語の文字列ではなく、単語のIDが入っていることに注意\n",
    "# バッチ内の次元は[センテンス内の単語サイズ, バッチサイズ]となっている\n",
    "print(f'バッチ内のTextフィールドのテンソルサイズ: {train_batch.Text.shape}')\n",
    "print(f'バッチのTextフィールドを確認: {train_batch.Text}')\n",
    "print()\n",
    "print(f'バッチ内のLabelフィールドのテンソルサイズ: {train_batch.Label.shape}')\n",
    "print(f'バッチのLabelフィールドを確認: {train_batch.Label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "近年言語処理界隈で様々な機械学習器のベースとなっているTransformerを用いて分類を行う<br>\n",
    "下の記事を読んでおくとコードの理解がしやすいと思います。\n",
    "- 元論文はこちら(https://arxiv.org/abs/1706.03762)\n",
    "- わかりやすい日本語解説(https://qiita.com/omiita/items/07e69aef6c156d23c538)\n",
    "![ImageError](https://pytorch.org/tutorials/_images/transformer_architecture.jpg)\n",
    "（出典: https://pytorch.org/tutorials/beginner/transformer_tutorial.html ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 機械学習器のモデルパラメータを定義\n",
    "from src.torch_model.transformer_model import TransformerModel  # モデルの書き方はソースコードを見てください\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPUかCPUか\n",
    "ntokens = len(text_field.vocab.stoi)  # トレーニングデータ内の語彙サイズ\n",
    "embed_size = 200  # 単語埋め込みベクトルの次元数\n",
    "nhid = 200  # ニューラルネットワークの隠れ層のユニット数\n",
    "nlayers = 1 # TransformerEncoder層の層数\n",
    "nhead = 1 # MultiHead Attention層の数\n",
    "dropout = 0.3 # ドロップアウトの割合\n",
    "is_classifier = True  # 分類問題の場合はTrue\n",
    "n_class = 21  # 分類するクラス数\n",
    "model = TransformerModel(ntokens, embed_size, nhead, nhid, nlayers, dropout, is_classifier, n_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレーニング用のパラメータを設定\n",
    "import math\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()  # 損失関数の定義（期待される出力との誤差を求めるものと思っていただければ良い）\n",
    "lr = 2.0  # learning rate(学習率)　大きければ大きいほど誤差を修正する勢いが強くなるが、学習が収束しないことがある\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)  # 最適化関数の種類 SGD: 確率的勾配降下法\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=1,  # 学習率の減衰を何ステップごとに行うか\n",
    "    gamma=0.9  # 減衰率\n",
    ")  # 学習回数経過による学習率減衰のスケジュール\n",
    "\n",
    "# トレーニング用スクリプト\n",
    "import time\n",
    "def train(log_interval: int = 50) -> None:\n",
    "    if log_interval < 1:\n",
    "        raise Exception('log_intervalは1以上の整数で指定')\n",
    "        return\n",
    "    model.train() # モデルをトレーニングモードに変更\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    # ミニバッチ単位でループ\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        optimizer.zero_grad()  # 重み更新量の初期化\n",
    "        output = model(batch.Text)  # ミニバッチのtext_fieldを入力\n",
    "        loss = criterion(output.view(-1, batch.Label.shape[0]).T, batch.Label)  # 誤差の計算\n",
    "        loss.backward()  # 誤差逆伝搬（バックプロパゲーション: BP）\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)  # BPの重み更新量の爆発を防ぐ正則化処理\n",
    "        optimizer.step()  # 重みの更新\n",
    "\n",
    "        total_loss += loss.item()  # ログ出力までの損失を合計\n",
    "        if i % log_interval == 0 and i > 0:\n",
    "            cur_loss = total_loss / log_interval  # ミニバッチ１つあたりの損失の平均\n",
    "            elapsed = time.time() - start_time  # 経過時間\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f}'.format(\n",
    "                    epoch, i, len(train_iter), scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "# 評価用スクリプト\n",
    "def evaluate(eval_model, batch_iterator) -> float:\n",
    "    eval_model.eval()  # モデルをトレーニングモードに変更\n",
    "    total_loss = 0.\n",
    "    counter = 0\n",
    "    with torch.no_grad():  # 勾配計算をしない\n",
    "        for i, batch in enumerate(batch_iterator):\n",
    "            output = eval_model(batch.Text)\n",
    "            output_flat = output.view(-1, batch.Label.shape[0]).T  # 出力されるテンソルの次元をbatch.Labelと損失計算できるように転置\n",
    "            total_loss += criterion(output_flat, batch.Label).item()\n",
    "            counter += 1\n",
    "    return total_loss / ((counter*batch_iterator.batch_size) - 1)  # 平均の損失値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルのトレーニング\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 30  # 学習回数\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(model, valid_iter)\n",
    "    print('-' * 89)\n",
    "    print(\n",
    "        '| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f}'.format(\n",
    "            epoch, (time.time() - epoch_start_time), val_loss\n",
    "        )\n",
    "    )\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習済みモデルの保存\n",
    "torch.save(best_model, './chABSA_clasifier_best.model')\n",
    "\n",
    "# vocabオブジェクトの保存（単語をIDに変換する辞書があるため）\n",
    "import pickle\n",
    "with open('./vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(text_field.vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataのLossの確認\n",
    "evaluate(best_model, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実際にテキストを入力して学習済みモデルの出力を確認する\n",
    "\n",
    "def predictor(model, dataset, top_k=3) -> None:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in dataset:\n",
    "            token_to_id_list = [text_field.vocab[token] for token in data.Text]  # 単語をIDに変換\n",
    "            tensor = torch.tensor([token_to_id_list], dtype=torch.long).T  # バッチサイズ1としてモデルに入力するためのテンソルを作成\n",
    "            output = model(tensor)\n",
    "            prob = nn.functional.softmax(output, dim=1)\n",
    "            val, idx = torch.topk(prob[0], k=prob.shape[1])  # 分類結果を確率順に取得\n",
    "            print(f'input text: {\"\".join(data.Text)}')\n",
    "            print(f'answer label: {data.Label}')\n",
    "            for i in range(top_k):\n",
    "                print(f'predict_{i+1}: {idx[i]}  (Probability: {(val[i] * 100):.2f}[%])')\n",
    "            print('---' * 10)\n",
    "    print('Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータセットの結果を見てみる\n",
    "predictor(model=best_model, dataset=test_dataset, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレーニングデータセットの結果を見てみる(100件)\n",
    "predictor(model=best_model, dataset=train_dataset[:100], top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### No.12（Challenge！!!）: モデルの精度を上げるための工夫を考える\n",
    "- No.11を参考にchABSA_datasetの分類精度を上げるために、どういった工夫が必要か考え、マークダウンで下のセルに回答してください。いくつ書いていただいても構いません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このセルに回答を記載してください。このセルはマークダウン形式になっています。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### No.13（Challenge！!!）: No.12であげていただいた工夫を実践してください\n",
    "- コードはコピペして書き換えていただいても構いませんし、イチから書いていただいても構いません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 以下に回答 ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
